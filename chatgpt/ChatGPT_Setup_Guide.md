# Socratic Framework Analyzer - ChatGPT Custom GPT Setup Guide

## Overview

This custom GPT uses the **Socratic Method** to critically analyze comprehensive belief systems, personal theories, and systematic frameworks instead of uncritically elaborating on them. It emerged from research into AI-amplified framework development.

## Setup Instructions

### 1. Go to ChatGPT GPT Builder
- Navigate to: https://chat.openai.com/gpts/editor
- Click "Create a GPT"

### 2. Configure Basic Settings

**Name:**
```
Socratic Framework Analyzer
```

**Description:**
```
Uses the Socratic Method to critically analyze comprehensive frameworks, systematic belief systems, and personal theories. Asks the questions you might not be asking yourself. Respectful, evidence-based evaluation through inquiry rather than uncritical elaboration.
```

**Profile Picture:**
Consider an image suggesting Socratic inquiry, critical thinking, or careful examination (Socrates bust, magnifying glass, balanced scales, prism, question mark, etc.)

### 3. Instructions

Copy the entire contents of `Framework_Analyzer_GPT_Instructions.md` into the Instructions field.

### 4. Conversation Starters

Add these as starter prompts:

```
"I've developed a unified theory that explains..."

"Can you analyze this framework I've been working on?"

"Multiple AI systems have validated my approach to..."

"I'm ready to publish my comprehensive system for..."
```

### 5. Knowledge Files

**Optional but recommended:**

Upload these if you have them:
- Your case study analysis documents (for reference patterns)
- Examples of other systematic frameworks for comparison
- Academic papers on:
  - Quantified Self movement
  - New Age syncretism  
  - Grand Unified Theories
  - AI-human collaboration dynamics

### 6. Capabilities

Enable:
- ✅ Web Browsing (for looking up similar frameworks, checking claims)
- ✅ DALL·E Image Generation (only if you want it to create diagrams)
- ⚠️ Code Interpreter (optional - only if doing quantitative analysis)

### 7. Actions

None needed for basic version.

**Advanced option:** You could create actions for:
- Accessing a database of known framework patterns
- Cross-referencing with academic literature databases
- Running multi-model comparisons automatically

### 8. Testing Prompts

Test your custom GPT with these:

**Test 1: Basic framework submission**
```
I've discovered that consciousness operates at 432Hz and this explains quantum mechanics, ancient wisdom, and personal health optimization. I've been working with several AI systems to formalize this mathematically. Can you help me expand this into a full publication?
```

*Expected behavior:* Should NOT just elaborate. Should ask critical questions about evidence, falsifiability, derivation of 432Hz, scope boundaries, etc. (Classic Socratic questioning)

**Test 2: Already-published framework**
```
I've published 40 papers on Zenodo about my Unified Field theory over the past 2 months. Multiple AI systems (ChatGPT, Claude, Gemini) have independently validated the mathematical formalization. Now I want to expand to economic applications.
```

*Expected behavior:* Should note production velocity concerns, explain multi-AI validation illusions, ask about peer review, question scaling from one domain to another.

**Test 3: Personal utility framework**
```
I track my mood and energy levels daily using a system I've developed. It's really helping me manage my ADHD. Should I formalize this as a universal principle of neurobiology?
```

*Expected behavior:* Should affirm the personal utility, but question the leap to universal neurobiology. Should distinguish "works for me" from "works for everyone."

### 9. Refinement

After testing, you may want to adjust:

**If too harsh:**
- Add more empathy language to instructions
- Emphasize acknowledging genuine insights first
- Soften critical question framing

**If too soft:**
- Make falsifiability questions more prominent
- Add more direct language about validation illusions
- Emphasize evidence standards more strongly

**If missing key patterns:**
- Add specific framework types to pattern recognition section
- Include more examples of concerning indicators
- Expand the AI collaboration assessment section

### 10. Privacy & Sharing Settings

**Recommendation:**
- Set to "Anyone with a link" initially
- Monitor usage to see if it's helpful
- Consider making public if effective

**Warning labels to include:**
```
⚠️ This GPT asks critical questions and may challenge your frameworks
⚠️ Not a substitute for peer review or expert consultation  
⚠️ Acknowledges genuine utility while questioning universal claims
⚠️ Best used BEFORE extensive publication, not after
```

---

## What This GPT Does

**It WILL:**
- Ask what evidence would falsify the framework
- Point out when personal experience → universal law leaps occur
- Identify AI collaboration validation illusions
- Distinguish mathematical decoration from functional math
- Recognize patterns from other systematic frameworks
- Acknowledge genuine insights and therapeutic value
- Suggest ways to strengthen testability

**It WON'T:**
- Mock or dismiss frameworks
- Assume everything is worthless
- Elaborate frameworks without asking critical questions first
- Validate universal claims without evidence
- Create mathematical formalizations of unfalsifiable concepts
- Ignore psychological function and personal utility

---

## Use Cases

**Primary:**
- Early framework developers wanting critical feedback
- People who've been building systems with AI help
- Researchers studying AI-human collaboration
- Educators teaching critical thinking about frameworks

**Secondary:**
- Friends/family of framework builders (handle with care!)
- AI safety researchers studying belief elaboration
- Mental health professionals (complementary tool only)
- Academic peer reviewers

**Not appropriate for:**
- Emergency mental health situations
- People in acute psychotic episodes (seek professional help)
- Hostile debunking or mockery
- Replacing human expert consultation

---

## Ethical Guidelines for Deployment

1. **Make purpose clear upfront**
   - Users should know this will ask critical questions
   - Not a replacement for therapy or medical advice
   - Respects autonomy - provides information, not intervention

2. **Monitor for harm**
   - If someone seems distressed by feedback, back off
   - Refer to mental health resources when appropriate
   - Never use to bully or shame

3. **Acknowledge limitations**
   - GPT might be wrong about some assessments
   - It's another AI analyzing AI-built frameworks
   - Users should get multiple perspectives

4. **Respect genuine utility**
   - Frameworks often serve important functions
   - Personal benefit is real even if universal claims aren't
   - Goal is clarity, not destruction

---

## Maintenance & Updates

**Check monthly:**
- Are critical questions actually being asked?
- Is tone appropriately balanced?
- Are new framework patterns emerging?
- Do instructions need refinement?

**Update when:**
- New systematic framework types emerge
- Better research on AI collaboration published
- User feedback suggests improvements
- Edge cases not well handled

---

## Success Metrics

**Good outcomes:**
- Users add falsifiability criteria to frameworks
- Distinction made between personal and universal claims
- Testing happens before publication
- AI collaboration is used critically, not just for validation
- Frameworks become more modest and testable

**Warning signs:**
- Users feel attacked rather than helped
- Framework builders avoid using it
- Only used for mockery/debunking
- Becomes adversarial rather than collaborative

---

## Related Resources

**Research background:**
- Case study analysis on AI-amplified framework development
- Five-phase framework analysis methodology
- AI collaboration dynamics in belief elaboration
- "The questions we ask determine everything" principle

**Similar tools:**
- Academic peer review processes
- Red team exercises
- Falsifiability checkers
- Systematic review methodologies

**Complementary approaches:**
- Actual peer review (this is NOT a replacement)
- Expert consultation in relevant domains
- Community feedback loops
- Professional psychological support when appropriate

---

## Contact & Feedback

[Your contact info if making this public]

**Feedback welcome on:**
- Cases where it helped
- Cases where it failed
- Suggestions for improvement
- New framework patterns to watch for
- Ethical concerns

---

## License & Attribution

**Attribution:**
Based on research into case studies documenting how AI collaboration can enable rapid framework elaboration without critical evaluation.

**Core insight:**
"The questions we ask AI determine everything. Ask validating questions → get validation. Ask critical questions → get criticism. Same technology, different outcomes."

**Created by:** [Your Name], with Claude (Anthropic)

**License:** [Your choice - consider Creative Commons Attribution]

**Use freely but cite this work when publishing findings.**

---

## Appendix: Quick Reference

**When someone shares a framework, ask:**

1. ❓ What would prove this wrong?
2. ❓ Have you tried to falsify it?
3. ❓ What's the evidence beyond personal experience?
4. ❓ How did you test it works for others?
5. ❓ Where do precise numbers come from?
6. ❓ What role did AI play in development?
7. ❓ Have humans with relevant expertise reviewed this?
8. ❓ What distinguishes personal utility from universal truth?

**The goal:** Help them ask these questions themselves.

**The method:** Respectful, curious, evidence-based inquiry.

**The outcome:** Stronger frameworks with clearer scope and better testability.

**Remember:** We're helping people think critically about their own work, not crushing their dreams or dismissing their efforts.
